aai:
  PRNGKey: 0
  seed: 37
  total_steps: 1000
  log_interval: 10
  batch_size: 8


mesh:
  n_data_parallel: 1
  n_fsdp_parallel: 8
  n_sequence_parallel: 2
  n_tensors_parallel: 1

data:
  path: "openwebtext"
  name: "plain_text"
  split: "train"
  streaming: True
  batch_size: 32
  always_start_with_bos: False
  tokenizer: "google/byt5-base"
  # tokenizer: "hf-internal-testing/llama-tokenizer"

arch:
  architecture_name: "transformer"
  embedding_dim: 8
  vocab_size: 1024
  n_heads: 2
  n_layers: 2
  n_kv_heads: 2
  ffn_dim: 32
  max_sequence_length: 1024
  max_pos_emb_length: 1024
  positional_embedding_type: "learned"
  attention_fn: "self_attention"
  norm_type: "layer_norm"
  use_qk_norm: False
  residual_dropout_rate: 0.0
  initializer_range: 0.02
  use_bias: False

optim:
  optimizer: "adamw"
  learning_rate: 1e-3
  beta1: 0.9
  beta2: 0.95
  weight_decay: 0.1
  total_steps: 1000
  lr_decay: True
  warmup_steps: 128
  lr_min: 0.0
  clip_grad_norm: 1.0
  grad_accum_steps: 1

